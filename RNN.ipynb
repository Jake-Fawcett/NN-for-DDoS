{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0b44c442a48bae290707df8e087ea8d315928ef76bfe5cc4dd19c6d12ddaf7fa6",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "b44c442a48bae290707df8e087ea8d315928ef76bfe5cc4dd19c6d12ddaf7fa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine Syn, Ldap and NetBios data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "chunksize = 10 ** 5\n",
    "\n",
    "synData = pd.DataFrame()\n",
    "ldapData = pd.DataFrame()\n",
    "netbiosData = pd.DataFrame()\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/Syn.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    synData = synData.append(chunk)\n",
    "\n",
    "data = data.append(synData)\n",
    "del synData\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/LDAP.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    ldapData = ldapData.append(chunk)\n",
    "\n",
    "data = data.append(ldapData)\n",
    "del ldapData\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/NetBIOS.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    netbiosData = netbiosData.append(chunk)\n",
    "\n",
    "data = data.append(netbiosData)\n",
    "del netbiosData\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Drop NaN and Inf values\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data = data.dropna()\n",
    "\n",
    "# # - - - - - - - - - -\n",
    "# Converting data to the right floats, removing unecessary fields\n",
    "# Convert int64 and str to float 64\n",
    "\n",
    "import ipaddress\n",
    "\n",
    "data.replace({'Syn': 1, 'NetBIOS': 1, 'LDAP': 1, 'BENIGN': 0}, inplace=True) # replace strings\n",
    "data[' Label'] = data[' Label'].astype(np.float64) # cast from int64 to float 64\n",
    "\n",
    "data['SimillarHTTP'] = data['SimillarHTTP'].astype(bool).astype(np.float64) # Replace non-zero with 1\n",
    "\n",
    "data.drop(['Unnamed: 0'], axis=1, inplace=True) # drop Unnamed: 0 because is just an ID\n",
    "data.drop(['Flow ID'], axis=1, inplace=True) # drop Flow ID because info is in other fields (its hard to encode too)\n",
    "data.drop([' Timestamp'], axis=1, inplace=True) # drop timestamp as we have them in order (its hard to encode too)\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].dtypes == np.int64:\n",
    "        data[column] = data[column].astype(np.float64)\n",
    "    elif data[column].dtypes == np.float64:\n",
    "        break\n",
    "    else:\n",
    "        for count, item in enumerate(data[column].values):\n",
    "            data[column].values[count] = np.float64(int(ipaddress.IPv4Address(item)))\n",
    "        data[column] = data[column].astype(np.float64)\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Scale the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler() \n",
    "\n",
    "columns = data.columns[:-1]\n",
    "\n",
    "data[columns] = scaler.fit_transform(data[columns])\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Now we split the data, 80% training, 20% testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(data, test_size=0.2, random_state=1) # Splits data into training and testing\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Here we create x_train, x_test, y_train, y_test as well as oversampling/undersampling data\n",
    "# due to the large difference in benign and other data\n",
    "print(df_train[' Label'].value_counts())\n",
    "count_class_1, count_class_0 = df_train[' Label'].value_counts()\n",
    "\n",
    "# divide df_train\n",
    "df_class_0 = df_train[df_train[' Label'] == 0]\n",
    "df_class_1 = df_train[df_train[' Label'] == 1]\n",
    "\n",
    "# Oversampling\n",
    "df_class_0_oversample = df_class_0.sample(round(count_class_1 / 10), replace=True)\n",
    "\n",
    "# Undersampling\n",
    "size_to_reduce_1_to = round(count_class_1 / 10)\n",
    "df_class_1_undersample = df_class_1.sample(size_to_reduce_1_to)\n",
    "\n",
    "df_train_over_under = pd.concat([df_class_1_undersample, df_class_0_oversample], axis=0)\n",
    "df_train = df_train_over_under.sample(frac=1, random_state=1)\n",
    "\n",
    "labels = df_train.columns[:-1]\n",
    "x_train = df_train[labels]\n",
    "y_train = df_train[' Label']\n",
    "\n",
    "print(\" - - - - - \")\n",
    "print('Random combined-sampling:')\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# - - - - - - - - - -\n",
    "\n",
    "test_count_class_1, test_count_class_0 = df_test[' Label'].value_counts()\n",
    "\n",
    "df_test_class_0 = df_test[df_test[' Label'] == 0]\n",
    "df_test_class_1 = df_test[df_test[' Label'] == 1]\n",
    "\n",
    "df_test_class_0_oversample = df_test_class_0.sample(round(test_count_class_1), replace=True)\n",
    "df_test_class_1_undersample = df_test_class_1.sample(round(test_count_class_0), replace=True)\n",
    "df_test_low_rate = pd.concat([df_test_class_1_undersample, df_test_class_0_oversample], axis=0)\n",
    "df_test_low_rate = df_test_low_rate.sample(frac=1, random_state=1)\n",
    "\n",
    "print(\" - - - - - \")\n",
    "print(\"Low rate:\")\n",
    "print(df_test_low_rate[' Label'].value_counts())\n",
    "x_test_low_rate = df_test_low_rate[labels]\n",
    "y_test_low_rate = df_test_low_rate[' Label']\n",
    "\n",
    "df_test = df_test.sample(frac=1, random_state=1)\n",
    "\n",
    "print(\" - - - - - \")\n",
    "print(\"High rate:\")\n",
    "print(df_test[' Label'].value_counts())\n",
    "x_test_high_rate = df_test[labels]\n",
    "y_test_high_rate = df_test[' Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 attack dataset\n",
    "\n",
    "print(x_train.shape)\n",
    "x_train = x_train.values.reshape(449128, 1, 84) \n",
    "\n",
    "print(x_test_low_rate.shape)\n",
    "x_test_low_rate = x_test_low_rate.values.reshape(568265, 1, 84) \n",
    "\n",
    "print(x_test_high_rate.shape)\n",
    "x_test_high_rate = x_test_high_rate.values.reshape(568265, 1, 84)\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train = y_train.values.reshape(449128, 1)\n",
    "\n",
    "\n",
    "print(y_test_low_rate.shape)\n",
    "y_test_low_rate = y_test_low_rate.values.reshape(568265, 1)\n",
    "\n",
    "print(y_test_high_rate.shape)\n",
    "y_test_high_rate = y_test_high_rate.values.reshape(568265, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size \t\t= 20   \n",
    "num_epochs \t\t= 100 \n",
    "\n",
    "input_dim = 84\n",
    "\n",
    "drop_prob_1 = 0.05\n",
    "drop_prob_2 = 0.1\n",
    "drop_prob_3 = 0.05\n",
    "\n",
    "dense_size_1 = 84\n",
    "dense_size_2 = 42\n",
    "lstm_size = 20\n",
    "dense_size_3 = 10\n",
    "dense_size_4 = 5\n",
    "dense_size_5 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Callback to stop when there is no loss or accuracy improvement in 3 epochs\n",
    "callback_loss = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "callback_accuracy = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# LSTM layer\n",
    "model.add(layers.Dense(dense_size_1, activation=\"relu\"))\n",
    "\n",
    "model.add(layers.Dropout(drop_prob_1))\n",
    "\n",
    "model.add(layers.LSTM(lstm_size, recurrent_dropout=drop_prob_2))\n",
    "\n",
    "model.add(layers.Dense(dense_size_3, activation=\"relu\"))\n",
    "\n",
    "model.add(layers.Dropout(drop_prob_3))\n",
    "\n",
    "model.add(layers.Dense(dense_size_4, activation=\"relu\"))\n",
    "\n",
    "model.add(layers.Dense(dense_size_5, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model using the training set\n",
    "# Validation_split indicates using fraction 0.1 (10%) for validation\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1, shuffle=True, callbacks=[callback_loss, callback_accuracy]) \n",
    "\n",
    "print(\"- - - Evaluation - - -\")\n",
    "print(\"High Rate\") # Evaluate the trained model on the high rate test set!\n",
    "model.evaluate(x_test_high_rate, y_test_high_rate, verbose=1)  \n",
    "\n",
    "print(\"Low Rate\") # Evaluate the trained model on the low rate test set!\n",
    "model.evaluate(x_test_low_rate, y_test_low_rate, verbose=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_high_rate = model.predict(x_test_high_rate)\n",
    "y_pred_low_rate = model.predict(x_test_low_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"low rate\")\n",
    "r2_value = r2_score(y_test_low_rate, y_pred_low_rate.round())\n",
    "print(r2_value)\n",
    "\n",
    "print(\"high rate\")\n",
    "r2_value = r2_score(y_test_high_rate, y_pred_high_rate.round())\n",
    "print(r2_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"low rate\")\n",
    "print(confusion_matrix(y_test_low_rate, y_pred_low_rate.round()))\n",
    "\n",
    "print(\"high rate\")\n",
    "print(confusion_matrix(y_test_high_rate, y_pred_high_rate.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "line1 = ax1.plot(history.history[\"loss\"])\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "line2 = ax2.plot(history.history[\"accuracy\"])\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and formatting the unknown (UDPLag) data in the same was as the training data\n",
    "\n",
    "udplagData = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/UDPLag.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    udplagData = udplagData.append(chunk)\n",
    "\n",
    "udplagData.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "udplagData = udplagData.dropna()\n",
    "\n",
    "import ipaddress\n",
    "\n",
    "print(udplagData[' Label'].value_counts())\n",
    "\n",
    "udplagData.replace({'UDP': 1, 'UDPLag': 1, 'Syn': 1, 'BENIGN': 0}, inplace=True)\n",
    "udplagData[' Label'] = udplagData[' Label'].astype(np.float64)\n",
    "\n",
    "udplagData['SimillarHTTP'] = udplagData['SimillarHTTP'].astype(bool).astype(np.float64)\n",
    "\n",
    "udplagData.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "udplagData.drop(['Flow ID'], axis=1, inplace=True)\n",
    "udplagData.drop([' Timestamp'], axis=1, inplace=True)\n",
    "\n",
    "for column in udplagData.columns:\n",
    "    if udplagData[column].dtypes == np.int64:\n",
    "        udplagData[column] = udplagData[column].astype(np.float64)\n",
    "    elif udplagData[column].dtypes == np.float64:\n",
    "        break\n",
    "    else:\n",
    "        for count, item in enumerate(udplagData[column].values):\n",
    "            udplagData[column].values[count] = np.float64(int(ipaddress.IPv4Address(item)))\n",
    "        udplagData[column] = udplagData[column].astype(np.float64)\n",
    "\n",
    "\n",
    "scaler = StandardScaler() \n",
    "columns = udplagData.columns[:-1]\n",
    "udplagData[columns] = scaler.fit_transform(udplagData[columns])\n",
    "\n",
    "\n",
    "test_count_class_1, test_count_class_0 = udplagData[' Label'].value_counts()\n",
    "\n",
    "udplag_data_class_0 = udplagData[udplagData[' Label'] == 0]\n",
    "udplag_data_class_1 = udplagData[udplagData[' Label'] == 1]\n",
    "\n",
    "udplag_data_class_0_oversample = udplag_data_class_0.sample(round(test_count_class_1), replace=True)\n",
    "udplag_data_class_1_undersample = udplag_data_class_1.sample(round(test_count_class_0), replace=True)\n",
    "udplag_data_low_rate = pd.concat([udplag_data_class_1_undersample, udplag_data_class_0_oversample], axis=0)\n",
    "udplag_data_low_rate = udplag_data_low_rate.sample(frac=1, random_state=1)\n",
    "\n",
    "print(\" - - - - - \")\n",
    "print(\"Low rate:\")\n",
    "print(udplag_data_low_rate[' Label'].value_counts())\n",
    "x_test_low_rate_udplag = udplag_data_low_rate[labels]\n",
    "y_test_low_rate_udplag = udplag_data_low_rate[' Label']\n",
    "\n",
    "udplagData = udplagData.sample(frac=1, random_state=1)\n",
    "\n",
    "print(\" - - - - - \")\n",
    "print(\"High rate:\")\n",
    "print(df_test[' Label'].value_counts())\n",
    "x_test_high_rate_udplag = udplagData[labels]\n",
    "y_test_high_rate_udplag = udplagData[' Label']\n",
    "\n",
    "\n",
    "print(x_test_high_rate_udplag.shape)\n",
    "x_test_high_rate_udplag = x_test_high_rate_udplag.values.reshape(674463, 1, 84)\n",
    "print(x_test_high_rate_udplag.shape)\n",
    "\n",
    "print(x_test_low_rate_udplag.shape)\n",
    "x_test_low_rate_udplag = x_test_low_rate_udplag.values.reshape(674463, 1, 84)\n",
    "print(x_test_low_rate_udplag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_high_rate_udplag = model.predict(x_test_high_rate_udplag)\n",
    "y_pred_low_rate_udplag = model.predict(x_test_low_rate_udplag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"low rate\")\n",
    "r2_value = r2_score(y_test_low_rate_udplag, y_pred_low_rate_udplag.round())\n",
    "print(r2_value)\n",
    "\n",
    "print(\"high rate\")\n",
    "r2_value = r2_score(y_test_high_rate_udplag, y_pred_high_rate_udplag.round())\n",
    "print(r2_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"low rate\")\n",
    "print(confusion_matrix(y_test_low_rate_udplag, y_pred_low_rate_udplag.round()))\n",
    "\n",
    "print(\"high rate\")\n",
    "print(confusion_matrix(y_test_high_rate_udplag, y_pred_high_rate_udplag.round()))"
   ]
  }
 ]
}