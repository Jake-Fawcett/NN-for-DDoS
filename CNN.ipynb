{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0b44c442a48bae290707df8e087ea8d315928ef76bfe5cc4dd19c6d12ddaf7fa6",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "b44c442a48bae290707df8e087ea8d315928ef76bfe5cc4dd19c6d12ddaf7fa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Jake\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3155: DtypeWarning: Columns (85) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "1.0    2245636\n",
      "0.0      27422\n",
      "Name:  Label, dtype: int64\n",
      "0.0    27422\n",
      "Name:  Label, dtype: int64\n",
      "1.0    2245636\n",
      "Name:  Label, dtype: int64\n",
      "Random combined-sampling:\n",
      "0.0    224564\n",
      "1.0    224564\n",
      "Name:  Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and combine Syn, Ldap and NetBios data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "chunksize = 10 ** 5\n",
    "\n",
    "synData = pd.DataFrame()\n",
    "ldapData = pd.DataFrame()\n",
    "netbiosData = pd.DataFrame()\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/Syn.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    synData = synData.append(chunk)\n",
    "\n",
    "data = data.append(synData)\n",
    "del synData\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/LDAP.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    ldapData = ldapData.append(chunk)\n",
    "\n",
    "data = data.append(ldapData)\n",
    "del ldapData\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/NetBIOS.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    netbiosData = netbiosData.append(chunk)\n",
    "\n",
    "data = data.append(netbiosData)\n",
    "del netbiosData\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Drop NaN and Inf values\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data = data.dropna()\n",
    "\n",
    "# # - - - - - - - - - -\n",
    "# Converting data to the right floats, removing unecessary fields\n",
    "# Convert int64 and str to float 64\n",
    "\n",
    "import ipaddress\n",
    "\n",
    "data.replace({'Syn': 1, 'NetBIOS': 1, 'LDAP': 1, 'BENIGN': 0}, inplace=True) # Replace strings\n",
    "data[' Label'] = data[' Label'].astype(np.float64) # Cast from int64 to float 64\n",
    "\n",
    "data['SimillarHTTP'] = data['SimillarHTTP'].astype(bool).astype(np.float64) # Replace non-zero with 1\n",
    "\n",
    "data.drop(['Unnamed: 0'], axis=1, inplace=True) # drop Unnamed: 0 because is just an ID\n",
    "data.drop(['Flow ID'], axis=1, inplace=True) # drop Flow ID because info is in other fields\n",
    "data.drop([' Timestamp'], axis=1, inplace=True) # drop timestamp as we have them in order, not necessary\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].dtypes == np.int64:\n",
    "        data[column] = data[column].astype(np.float64)\n",
    "    elif data[column].dtypes == np.float64:\n",
    "        break\n",
    "    else:\n",
    "        for count, item in enumerate(data[column].values):\n",
    "            data[column].values[count] = np.float64(int(ipaddress.IPv4Address(item)))\n",
    "        data[column] = data[column].astype(np.float64)\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Scale the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler() \n",
    "\n",
    "columns = data.columns[:-1]\n",
    "\n",
    "data[columns] = scaler.fit_transform(data[columns])\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Split the data into 80% training, 20% testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(data, test_size=0.2, random_state=1)\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Here we create x_train, x_test, y_train, y_test as well as oversampling/undersampling data\n",
    "# due to the large difference in benign and other data\n",
    "\n",
    "print(df_train[' Label'].value_counts())\n",
    "count_class_1, count_class_0 = df_train[' Label'].value_counts()\n",
    "\n",
    "# divide df_train\n",
    "df_class_0 = df_train[df_train[' Label'] == 0]\n",
    "df_class_1 = df_train[df_train[' Label'] == 1]\n",
    "\n",
    "print(df_class_0[' Label'].value_counts())\n",
    "print(df_class_1[' Label'].value_counts())\n",
    "\n",
    "# Oversampling\n",
    "df_class_0_oversample = df_class_0.sample(round(count_class_1 / 10), replace=True)\n",
    "\n",
    "# Undersampling\n",
    "size_to_reduce_1_to = round(count_class_1 / 10)\n",
    "df_class_1_undersample = df_class_1.sample(size_to_reduce_1_to)\n",
    "count_class_1 = size_to_reduce_1_to\n",
    "\n",
    "df_train_over_under = pd.concat([df_class_1_undersample, df_class_0_oversample], axis=0)\n",
    "df_train = df_train_over_under\n",
    "\n",
    "labels = df_train.columns[:-1]\n",
    "x_train = df_train[labels]\n",
    "y_train = df_train[' Label']\n",
    "\n",
    "x_test = df_test[labels]\n",
    "y_test = df_test[' Label']\n",
    "\n",
    "print('Random combined-sampling:')\n",
    "print(df_train_over_under[' Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(449128, 84)\n(449128, 12, 7)\n(568265, 84)\n(568265, 12, 7)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data to be suitable for CNN, (12 by 7 'image' shape)\n",
    "print(x_train.shape)\n",
    "x_train_reshaped = x_train.values.reshape(449128, 12, 7)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "x_test_reshaped = x_test.values.reshape(568265, 12, 7)\n",
    "print(x_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "449128\n12\n7\n"
     ]
    }
   ],
   "source": [
    "num_train, height, width = x_train_reshaped.shape\n",
    "print(num_train)\n",
    "print(height)\n",
    "print(width)\n",
    "\n",
    "num_classes = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size \t\t= 20\n",
    "num_epochs \t\t= 1\n",
    "\n",
    "kernel_size \t= 3\n",
    "pool_size \t\t= 2\n",
    "conv_depth_1 \t= 20\n",
    "conv_depth_2 \t= 32\n",
    "\n",
    "drop_prob_1 \t= 0.05\n",
    "drop_prob_2 \t= 0.1\n",
    "\n",
    "hidden_size \t= 10  \n",
    "hidden_size2 \t= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, AveragePooling2D, Dense, Dropout, Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "# Callback to stop when there is no loss or accuracy improvement in 3 epochs\n",
    "callback_loss = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "callback_accuracy = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)\n",
    "\n",
    "inp = Input(shape=(height, width, 1)) \n",
    "\n",
    "# CNN\n",
    "conv_1 = Convolution2D(conv_depth_1, (kernel_size, kernel_size), padding='same', activation='relu')(inp)\n",
    "conv_2 = Convolution2D(conv_depth_1, (kernel_size, kernel_size), padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "\n",
    "\n",
    "# Flatten and Fully-connected layer\n",
    "flat = Flatten()(drop_1)\n",
    "\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "\n",
    "hidden2 = Dense(hidden_size2, activation='relu')(drop_3)\n",
    "\n",
    "out = Dense(num_classes, activation='sigmoid')(hidden2)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) \n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training set\n",
    "# Validation_split indicates using fraction 0.1 (10%) for validation\n",
    "history =  model.fit(x_train_reshaped, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1, shuffle=True, callbacks=[callback_loss, callback_accuracy])\n",
    "\n",
    "print(\"- - - Evaluation - - -\")\n",
    "model.evaluate(x_test_reshaped, y_test, verbose=1)  # Evaluate the trained model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_value = r2_score(y_test, y_pred.round())\n",
    "print(r2_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "line1 = ax1.plot(history.history[\"loss\"])\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "line2 = ax2.plot(history.history[\"accuracy\"])\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and formatting the unknown (UDPLag) data in the same was as the training data\n",
    "\n",
    "udplagData = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/UDPLag.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    udplagData = udplagData.append(chunk)\n",
    "\n",
    "udplagData.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "udplagData = udplagData.dropna()\n",
    "\n",
    "import ipaddress\n",
    "\n",
    "print(udplagData[' Label'].value_counts())\n",
    "\n",
    "udplagData.replace({'UDP': 1, 'UDPLag': 1, 'Syn': 1, 'BENIGN': 0}, inplace=True)\n",
    "udplagData[' Label'] = udplagData[' Label'].astype(np.float64)\n",
    "\n",
    "udplagData['SimillarHTTP'] = udplagData['SimillarHTTP'].astype(bool).astype(np.float64)\n",
    "\n",
    "udplagData.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "udplagData.drop(['Flow ID'], axis=1, inplace=True)\n",
    "udplagData.drop([' Timestamp'], axis=1, inplace=True)\n",
    "\n",
    "for column in udplagData.columns:\n",
    "    if udplagData[column].dtypes == np.int64:\n",
    "        udplagData[column] = udplagData[column].astype(np.float64)\n",
    "    elif udplagData[column].dtypes == np.float64:\n",
    "        break\n",
    "    else:\n",
    "        for count, item in enumerate(udplagData[column].values):\n",
    "            udplagData[column].values[count] = np.float64(int(ipaddress.IPv4Address(item)))\n",
    "        udplagData[column] = udplagData[column].astype(np.float64)\n",
    "\n",
    "\n",
    "scaler = StandardScaler() \n",
    "columns = udplagData.columns[:-1]\n",
    "udplagData[columns] = scaler.fit_transform(udplagData[columns])\n",
    "\n",
    "x_test_udplag = udplagData[labels]\n",
    "y_test_udplag = udplagData[' Label']\n",
    "\n",
    "print(x_test_udplag.shape)\n",
    "x_test_udplag_reshaped = x_test_udplag.values.reshape(674463, 12, 7)\n",
    "print(x_test_udplag_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_udplag = model.predict(x_test_udplag_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_value = r2_score(y_test_udplag, y_pred_udplag.round())\n",
    "print(r2_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test_udplag, y_pred_udplag.round()))"
   ]
  }
 ]
}