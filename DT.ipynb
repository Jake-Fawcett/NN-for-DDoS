{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0b44c442a48bae290707df8e087ea8d315928ef76bfe5cc4dd19c6d12ddaf7fa6",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "b44c442a48bae290707df8e087ea8d315928ef76bfe5cc4dd19c6d12ddaf7fa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine Syn, Ldap and NetBios data\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "chunksize = 10 ** 5\r\n",
    "\r\n",
    "synData = pd.DataFrame()\r\n",
    "ldapData = pd.DataFrame()\r\n",
    "netbiosData = pd.DataFrame()\r\n",
    "data = pd.DataFrame()\r\n",
    "\r\n",
    "for chunk in pd.read_csv(\"data/03-11/Syn.csv\", chunksize=chunksize, nrows=1000000):\r\n",
    "    synData = synData.append(chunk)\r\n",
    "\r\n",
    "data = data.append(synData)\r\n",
    "del synData\r\n",
    "\r\n",
    "for chunk in pd.read_csv(\"data/03-11/LDAP.csv\", chunksize=chunksize, nrows=1000000):\r\n",
    "    ldapData = ldapData.append(chunk)\r\n",
    "\r\n",
    "data = data.append(ldapData)\r\n",
    "del ldapData\r\n",
    "\r\n",
    "for chunk in pd.read_csv(\"data/03-11/NetBIOS.csv\", chunksize=chunksize, nrows=1000000):\r\n",
    "    netbiosData = netbiosData.append(chunk)\r\n",
    "\r\n",
    "data = data.append(netbiosData)\r\n",
    "del netbiosData\r\n",
    "\r\n",
    "# - - - - - - - - - -\r\n",
    "# Drop NaN and Inf values\r\n",
    "\r\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\r\n",
    "data = data.dropna()\r\n",
    "\r\n",
    "# # - - - - - - - - - -\r\n",
    "# Converting data to the right floats, removing unecessary fields\r\n",
    "# Convert int64 and str to float 64\r\n",
    "\r\n",
    "import ipaddress\r\n",
    "\r\n",
    "data.replace({'Syn': 1, 'NetBIOS': 1, 'LDAP': 1, 'BENIGN': 0}, inplace=True) # Replace strings\r\n",
    "data[' Label'] = data[' Label'].astype(np.float64) # Cast from int64 to float 64\r\n",
    "\r\n",
    "data['SimillarHTTP'] = data['SimillarHTTP'].astype(bool).astype(np.float64) # Replace non-zero with 1\r\n",
    "\r\n",
    "data.drop(['Unnamed: 0'], axis=1, inplace=True) # drop Unnamed: 0 because is just an ID\r\n",
    "data.drop(['Flow ID'], axis=1, inplace=True) # drop Flow ID because info is in other fields\r\n",
    "data.drop([' Timestamp'], axis=1, inplace=True) # drop timestamp as we have them in order, not necessary\r\n",
    "\r\n",
    "for column in data.columns:\r\n",
    "    if data[column].dtypes == np.int64:\r\n",
    "        data[column] = data[column].astype(np.float64)\r\n",
    "    elif data[column].dtypes == np.float64:\r\n",
    "        break\r\n",
    "    else:\r\n",
    "        for count, item in enumerate(data[column].values):\r\n",
    "            data[column].values[count] = np.float64(int(ipaddress.IPv4Address(item)))\r\n",
    "        data[column] = data[column].astype(np.float64)\r\n",
    "\r\n",
    "# - - - - - - - - - -\r\n",
    "# Scale the data\r\n",
    "\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "scaler = StandardScaler() \r\n",
    "\r\n",
    "columns = data.columns[:-1]\r\n",
    "\r\n",
    "data[columns] = scaler.fit_transform(data[columns])\r\n",
    "\r\n",
    "# - - - - - - - - - -\r\n",
    "# Split the data into 80% training, 20% testing\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "df_train, df_test = train_test_split(data, test_size=0.2, random_state=1)\r\n",
    "\r\n",
    "# - - - - - - - - - -\r\n",
    "# Here we create x_train, x_test, y_train, y_test as well as oversampling/undersampling data\r\n",
    "# due to the large difference in benign and other data\r\n",
    "\r\n",
    "print(df_train[' Label'].value_counts())\r\n",
    "count_class_1, count_class_0 = df_train[' Label'].value_counts()\r\n",
    "\r\n",
    "# divide df_train\r\n",
    "df_class_0 = df_train[df_train[' Label'] == 0]\r\n",
    "df_class_1 = df_train[df_train[' Label'] == 1]\r\n",
    "\r\n",
    "print(df_class_0[' Label'].value_counts())\r\n",
    "print(df_class_1[' Label'].value_counts())\r\n",
    "\r\n",
    "# Oversampling\r\n",
    "df_class_0_oversample = df_class_0.sample(round(count_class_1 / 10), replace=True)\r\n",
    "\r\n",
    "# Undersampling\r\n",
    "size_to_reduce_1_to = round(count_class_1 / 10)\r\n",
    "df_class_1_undersample = df_class_1.sample(size_to_reduce_1_to)\r\n",
    "count_class_1 = size_to_reduce_1_to\r\n",
    "\r\n",
    "df_train_over_under = pd.concat([df_class_1_undersample, df_class_0_oversample], axis=0)\r\n",
    "df_train = df_train_over_under\r\n",
    "\r\n",
    "labels = df_train.columns[:-1]\r\n",
    "x_train = df_train[labels]\r\n",
    "y_train = df_train[' Label']\r\n",
    "\r\n",
    "x_test = df_test[labels]\r\n",
    "y_test = df_test[' Label']\r\n",
    "\r\n",
    "print('Random combined-sampling:')\r\n",
    "print(df_train_over_under[' Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,1:].values\n",
    "y = data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regr = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "regr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"- - - Known Data - - -\")\n",
    "r2_value = r2_score(y_test, y_pred.round())\n",
    "print(r2_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and formatting the unknown (UDPLag) data in the same was as the training data\n",
    "\n",
    "udplagData = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/UDPLag.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    udplagData = udplagData.append(chunk)\n",
    "\n",
    "udplagData.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "udplagData = udplagData.dropna()\n",
    "\n",
    "import ipaddress\n",
    "\n",
    "print(udplagData[' Label'].value_counts())\n",
    "\n",
    "udplagData.replace({'UDP': 1, 'UDPLag': 1, 'Syn': 1, 'BENIGN': 0}, inplace=True)\n",
    "udplagData[' Label'] = udplagData[' Label'].astype(np.float64)\n",
    "\n",
    "udplagData['SimillarHTTP'] = udplagData['SimillarHTTP'].astype(bool).astype(np.float64)\n",
    "\n",
    "udplagData.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "udplagData.drop(['Flow ID'], axis=1, inplace=True)\n",
    "udplagData.drop([' Timestamp'], axis=1, inplace=True)\n",
    "\n",
    "for column in udplagData.columns:\n",
    "    if udplagData[column].dtypes == np.int64:\n",
    "        udplagData[column] = udplagData[column].astype(np.float64)\n",
    "    elif udplagData[column].dtypes == np.float64:\n",
    "        break\n",
    "    else:\n",
    "        for count, item in enumerate(udplagData[column].values):\n",
    "            udplagData[column].values[count] = np.float64(int(ipaddress.IPv4Address(item)))\n",
    "        udplagData[column] = udplagData[column].astype(np.float64)\n",
    "\n",
    "\n",
    "scaler = StandardScaler() \n",
    "columns = udplagData.columns[:-1]\n",
    "udplagData[columns] = scaler.fit_transform(udplagData[columns])\n",
    "\n",
    "x_test_udplag = udplagData[labels]\n",
    "y_test_udplag = udplagData[' Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr.predict(x_test_udplag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- - - Unknown Data - - -\")\n",
    "r2_value = r2_score(y_test_udplag, y_pred_udplag.round())\n",
    "print(r2_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test_udplag, y_pred.round()))"
   ]
  }
 ]
}