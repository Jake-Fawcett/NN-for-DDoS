{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0b44c442a48bae290707df8e087ea8d315928ef76bfe5cc4dd19c6d12ddaf7fa6",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "b44c442a48bae290707df8e087ea8d315928ef76bfe5cc4dd19c6d12ddaf7fa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine Syn, Ldap and NetBios data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "chunksize = 10 ** 5\n",
    "\n",
    "synData = pd.DataFrame()\n",
    "ldapData = pd.DataFrame()\n",
    "netbiosData = pd.DataFrame()\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/Syn.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    synData = synData.append(chunk)\n",
    "\n",
    "data = data.append(synData)\n",
    "del synData\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/LDAP.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    ldapData = ldapData.append(chunk)\n",
    "\n",
    "data = data.append(ldapData)\n",
    "del ldapData\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/NetBIOS.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    netbiosData = netbiosData.append(chunk)\n",
    "\n",
    "data = data.append(netbiosData)\n",
    "del netbiosData\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Drop NaN and Inf values\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data = data.dropna()\n",
    "\n",
    "# # - - - - - - - - - -\n",
    "# Converting data to the right floats, removing unecessary fields\n",
    "# Convert int64 and str to float 64\n",
    "\n",
    "import ipaddress\n",
    "\n",
    "data.replace({'Syn': 1, 'NetBIOS': 1, 'LDAP': 1, 'BENIGN': 0}, inplace=True) # Replace strings\n",
    "data[' Label'] = data[' Label'].astype(np.float64) # Cast from int64 to float 64\n",
    "\n",
    "data['SimillarHTTP'] = data['SimillarHTTP'].astype(bool).astype(np.float64) # Replace non-zero with 1\n",
    "\n",
    "data.drop(['Unnamed: 0'], axis=1, inplace=True) # drop Unnamed: 0 because is just an ID\n",
    "data.drop(['Flow ID'], axis=1, inplace=True) # drop Flow ID because info is in other fields\n",
    "data.drop([' Timestamp'], axis=1, inplace=True) # drop timestamp as we have them in order, not necessary\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].dtypes == np.int64:\n",
    "        data[column] = data[column].astype(np.float64)\n",
    "    elif data[column].dtypes == np.float64:\n",
    "        break\n",
    "    else:\n",
    "        for count, item in enumerate(data[column].values):\n",
    "            data[column].values[count] = np.float64(int(ipaddress.IPv4Address(item)))\n",
    "        data[column] = data[column].astype(np.float64)\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Scale the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler() \n",
    "\n",
    "columns = data.columns[:-1]\n",
    "\n",
    "data[columns] = scaler.fit_transform(data[columns])\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Split the data into 80% training, 20% testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(data, test_size=0.2, random_state=1)\n",
    "\n",
    "# - - - - - - - - - -\n",
    "# Here we create x_train, x_test, y_train, y_test as well as oversampling/undersampling data\n",
    "# due to the large difference in benign and other data\n",
    "\n",
    "print(df_train[' Label'].value_counts())\n",
    "count_class_1, count_class_0 = df_train[' Label'].value_counts()\n",
    "\n",
    "# divide df_train\n",
    "df_class_0 = df_train[df_train[' Label'] == 0]\n",
    "df_class_1 = df_train[df_train[' Label'] == 1]\n",
    "\n",
    "print(df_class_0[' Label'].value_counts())\n",
    "print(df_class_1[' Label'].value_counts())\n",
    "\n",
    "# Oversampling\n",
    "df_class_0_oversample = df_class_0.sample(round(count_class_1 / 10), replace=True)\n",
    "\n",
    "# Undersampling\n",
    "size_to_reduce_1_to = round(count_class_1 / 10)\n",
    "df_class_1_undersample = df_class_1.sample(size_to_reduce_1_to)\n",
    "count_class_1 = size_to_reduce_1_to\n",
    "\n",
    "df_train_over_under = pd.concat([df_class_1_undersample, df_class_0_oversample], axis=0)\n",
    "df_train = df_train_over_under\n",
    "\n",
    "labels = df_train.columns[:-1]\n",
    "x_train = df_train[labels]\n",
    "y_train = df_train[' Label']\n",
    "\n",
    "x_test = df_test[labels]\n",
    "y_test = df_test[' Label']\n",
    "\n",
    "print('Random combined-sampling:')\n",
    "print(df_train_over_under[' Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import sklearn.model_selection as model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rbf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"- - - Known Data - - -\")\n",
    "r2_value = r2_score(y_test, y_pred.round())\n",
    "print(r2_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and formatting the unknown (UDPLag) data in the same was as the training data\n",
    "\n",
    "udplagData = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(\"data/03-11/UDPLag.csv\", chunksize=chunksize, nrows=1000000):\n",
    "    udplagData = udplagData.append(chunk)\n",
    "\n",
    "udplagData.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "udplagData = udplagData.dropna()\n",
    "\n",
    "import ipaddress\n",
    "\n",
    "print(udplagData[' Label'].value_counts())\n",
    "\n",
    "udplagData.replace({'UDP': 1, 'UDPLag': 1, 'Syn': 1, 'BENIGN': 0}, inplace=True)\n",
    "udplagData[' Label'] = udplagData[' Label'].astype(np.float64)\n",
    "\n",
    "udplagData['SimillarHTTP'] = udplagData['SimillarHTTP'].astype(bool).astype(np.float64)\n",
    "\n",
    "udplagData.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "udplagData.drop(['Flow ID'], axis=1, inplace=True)\n",
    "udplagData.drop([' Timestamp'], axis=1, inplace=True)\n",
    "\n",
    "for column in udplagData.columns:\n",
    "    if udplagData[column].dtypes == np.int64:\n",
    "        udplagData[column] = udplagData[column].astype(np.float64)\n",
    "    elif udplagData[column].dtypes == np.float64:\n",
    "        break\n",
    "    else:\n",
    "        for count, item in enumerate(udplagData[column].values):\n",
    "            udplagData[column].values[count] = np.float64(int(ipaddress.IPv4Address(item)))\n",
    "        udplagData[column] = udplagData[column].astype(np.float64)\n",
    "\n",
    "\n",
    "scaler = StandardScaler() \n",
    "columns = udplagData.columns[:-1]\n",
    "udplagData[columns] = scaler.fit_transform(udplagData[columns])\n",
    "\n",
    "x_test_udplag = udplagData[labels]\n",
    "y_test_udplag = udplagData[' Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rbf.predict(x_test_udplag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- - - Unknown Data - - -\")\n",
    "r2_value = r2_score(y_test_udplag, y_pred_udplag.round())\n",
    "print(r2_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test_udplag, y_pred.round()))"
   ]
  }
 ]
}